{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95e61214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6b1a793",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'titanic_train_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Check if the file exists\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(\u001b[43mtitanic_train_path\u001b[49m):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError: train.csv not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitanic_train_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPlease ensure you have downloaded and unzipped titanic.zip,\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'titanic_train_path' is not defined"
     ]
    }
   ],
   "source": [
    "# Check if the file exists\n",
    "if not os.path.exists(titanic_train_path):\n",
    "    print(f\"Error: train.csv not found at {titanic_train_path}\")\n",
    "    print(\"Please ensure you have downloaded and unzipped titanic.zip,\")\n",
    "    print(\"and placed train.csv into the 'data/raw/' directory of your project.\")\n",
    "else:\n",
    "    # Load the dataset\n",
    "    df_titanic = pd.read_csv(titanic_train_path)\n",
    "    print(\"Titanic train.csv data loaded successfully!\")\n",
    "    print(f\"Dataset shape: {df_titanic.shape} (rows, columns)\")\n",
    "\n",
    "    # Display the first few rows of the data\n",
    "    print(\"\\nFirst 5 rows of the dataset:\")\n",
    "    print(df_titanic.head())\n",
    "\n",
    "    # Display basic information about the dataset (column names, non-null counts, data types)\n",
    "    print(\"\\nDataset Information:\")\n",
    "    df_titanic.info()\n",
    "\n",
    "    # Display descriptive statistics for numerical columns\n",
    "    print(\"\\nDescriptive statistics for numerical columns:\")\n",
    "    print(df_titanic.describe())\n",
    "\n",
    "    # Check the distribution of the target variable 'Survived'\n",
    "    print(\"\\nDistribution of 'Survived' column:\")\n",
    "    print(df_titanic['Survived'].value_counts())\n",
    "    print(\"0: Not Survived, 1: Survived\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3dac727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numerical features before preprocessing: ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
      "Categorical features before preprocessing: ['Sex', 'Embarked']\n",
      "\n",
      "Training set features shape (original): (712, 7)\n",
      "Test set features shape (original): (179, 7)\n",
      "\n",
      "Data preprocessing complete!\n",
      "Processed training set features shape: (712, 10)\n",
      "Number of processed feature names: 10\n",
      "Example of processed feature names (first 10): ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15239/2897246885.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_processed['Embarked'].fillna(most_frequent_embarked, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df_titanic is the DataFrame loaded in Step 1\n",
    "\n",
    "# Create a copy of the data to avoid modifying the original DataFrame\n",
    "df_processed = df_titanic.copy()\n",
    "\n",
    "# 1. Handle Missing Values\n",
    "\n",
    "# 'Age' column: Impute with median, as age distribution might not be normal, and median is robust to outliers\n",
    "age_imputer = SimpleImputer(strategy='median')\n",
    "df_processed['Age'] = age_imputer.fit_transform(df_processed[['Age']])\n",
    "\n",
    "# 'Embarked' column: Impute with mode, as it is a categorical feature\n",
    "# First, find the mode\n",
    "most_frequent_embarked = df_processed['Embarked'].mode()[0]\n",
    "df_processed['Embarked'].fillna(most_frequent_embarked, inplace=True)\n",
    "\n",
    "# 'Cabin' column: Too many missing values, and the feature itself might be too complex for simple models.\n",
    "# Typically dropped or converted to a binary \"has_cabin_info\" feature.\n",
    "# For this task, we choose to drop it directly.\n",
    "df_processed.drop('Cabin', axis=1, inplace=True)\n",
    "\n",
    "# 2. Feature Selection\n",
    "# Remove 'PassengerId', 'Name', 'Ticket' as they are not directly relevant for classification\n",
    "# 'Survived' is the target variable and should also be removed from features\n",
    "features_to_drop = ['PassengerId', 'Name', 'Ticket']\n",
    "df_features = df_processed.drop(columns=features_to_drop + ['Survived'])\n",
    "target = df_processed['Survived']\n",
    "\n",
    "# 3. Identify Numerical and Categorical Features\n",
    "numeric_features = df_features.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = df_features.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "print(f\"\\nNumerical features before preprocessing: {numeric_features}\")\n",
    "print(f\"Categorical features before preprocessing: {categorical_features}\")\n",
    "\n",
    "# 4. Build Preprocessing Pipelines\n",
    "# Numerical feature processing: Standardization\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()) # Standardize numerical features\n",
    "])\n",
    "\n",
    "# Categorical feature processing: One-hot encoding\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')) # One-hot encode categorical features\n",
    "])\n",
    "\n",
    "# Use ColumnTransformer to apply different preprocessing steps to different types of features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# 5. Split data into training and testing sets\n",
    "# test_size=0.2 means 20% of the data is used for testing, random_state ensures reproducibility\n",
    "# stratify=target ensures that the proportion of 'Survived' in training and testing sets is similar to the original data, which is important for imbalanced datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features, target, test_size=0.2, random_state=42, stratify=target)\n",
    "\n",
    "print(f\"\\nTraining set features shape (original): {X_train.shape}\")\n",
    "print(f\"Test set features shape (original): {X_test.shape}\")\n",
    "\n",
    "# Apply the preprocessor to the training data and transform it\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "# Apply the preprocessor to the test data (only transform, do not refit)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Get the names of the processed features\n",
    "# For one-hot encoded categorical features, names will be expanded\n",
    "processed_feature_names = numeric_features + \\\n",
    "                          list(preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features))\n",
    "\n",
    "print(\"\\nData preprocessing complete!\")\n",
    "print(f\"Processed training set features shape: {X_train_processed.shape}\")\n",
    "print(f\"Number of processed feature names: {len(processed_feature_names)}\")\n",
    "print(\"Example of processed feature names (first 10):\", processed_feature_names[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e0fc95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
