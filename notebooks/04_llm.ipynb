{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec903ea0",
   "metadata": {},
   "source": [
    "# Demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ff6a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8b28250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: huggingface_hub in /home/vscode/.local/lib/python3.12/site-packages (1.1.2)\n",
      "Requirement already satisfied: filelock in /home/vscode/.local/lib/python3.12/site-packages (from huggingface_hub) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/vscode/.local/lib/python3.12/site-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /home/vscode/.local/lib/python3.12/site-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/vscode/.local/lib/python3.12/site-packages (from huggingface_hub) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/vscode/.local/lib/python3.12/site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/vscode/.local/lib/python3.12/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: shellingham in /home/vscode/.local/lib/python3.12/site-packages (from huggingface_hub) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/vscode/.local/lib/python3.12/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in /home/vscode/.local/lib/python3.12/site-packages (from huggingface_hub) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vscode/.local/lib/python3.12/site-packages (from huggingface_hub) (4.14.1)\n",
      "Requirement already satisfied: anyio in /home/vscode/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface_hub) (4.9.0)\n",
      "Requirement already satisfied: certifi in /home/vscode/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface_hub) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /home/vscode/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface_hub) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/vscode/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /home/vscode/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub) (0.16.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/vscode/.local/lib/python3.12/site-packages (from typer-slim->huggingface_hub) (8.2.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/vscode/.local/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e48dfe1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid!\n",
      "{'type': 'user', 'id': '690c782b935e19d8d3215a9b', 'name': 'Lei31', 'fullname': 'Gao', 'isPro': False, 'avatarUrl': '/avatars/2518e1d33e83acd77681c9a7886d519a.svg', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'sdp-llm', 'role': 'fineGrained', 'createdAt': '2025-11-06T10:35:51.307Z', 'fineGrained': {'canReadGatedRepos': True, 'global': [], 'scoped': [{'entity': {'_id': '690c782b935e19d8d3215a9b', 'type': 'user', 'name': 'Lei31'}, 'permissions': ['repo.content.read', 'repo.write', 'inference.serverless.write', 'inference.endpoints.infer.write', 'inference.endpoints.write', 'user.webhooks.read', 'user.webhooks.write', 'collection.read', 'collection.write', 'job.write']}]}}}}\n",
      "Access granted to LLAMA model!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "with open(\"/home/vscode/.cache/huggingface/token\", \"r\") as token_file:\n",
    "    token = token_file.read().strip()\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "response = requests.get(\"https://huggingface.co/api/whoami-v2\", headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Token is valid!\")\n",
    "    print(response.json())\n",
    "else:\n",
    "    print(\"Invalid token:\", response.status_code)\n",
    "\n",
    "# 验证模型权限\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "response = requests.get(\"https://huggingface.co/api/models/meta-llama/Llama-2-7b-hf\", headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Access granted to LLAMA model!\")\n",
    "elif response.status_code == 403:\n",
    "    print(\"Access denied. Please request access.\")\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e0edaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.9646604061126709}]\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "#pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-2-7b-hf\", use_auth_token=token, revision=\"main\"  )\n",
    "pipe = pipeline(\"text-classification\", model=\"distilgpt2\")\n",
    "text = \"I love this product!\"\n",
    "result = pipe(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77b19098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b691c05d7ec74569ba385b0209945434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff74eb6bdde744808704280bcb5ae937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee09cfc570274f849c26bfbf9af4f372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a21fe3136e41c5b103ff3891f21edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f04d9614fa146d388e0fade931f2e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56ffb4bad194c4281dcfc8323fadf02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Once upon a time there was a man named Joseph, and he lived in two places at once: in his own house, and in a large tree.\\n\\nThe most beautiful thing in the world was to him; so he took his sister and'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "#nlp = pipeline(\"text-classification\", model=\"meta-llama/Llama-2-7b-hf\")\n",
    "nlp = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-1.3B\")\n",
    "text = \"Once upon a time\"\n",
    "result = nlp(text, max_length=50, do_sample=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9d2759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
